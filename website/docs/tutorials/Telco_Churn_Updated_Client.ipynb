{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telco Churn Prediction with Refactored Xplainable Client\n",
    "\n",
    "This notebook demonstrates the **complete ML workflow** using the new refactored Xplainable client with:\n",
    "\n",
    "- **Type-safe Pydantic models**  \n",
    "- **Comprehensive error handling**  \n",
    "- **Service-oriented architecture**  \n",
    "- **All new client endpoints**  \n",
    "- **Robust production patterns**  \n",
    "\n",
    "We'll predict customer churn using the IBM Telco dataset while showcasing:\n",
    "- **Models Service**: Type-safe model creation and management\n",
    "- **Deployments Service**: Model deployment with proper error handling\n",
    "- **Preprocessing Service**: Pipeline management and data transformation\n",
    "- **Autotrain Service**: AI-powered automated training workflows\n",
    "- **Inference Service**: Predictions and explanations\n",
    "- **GPT Service**: AI-generated insights and reports\n",
    "- **Datasets Service**: Data management and loading\n",
    "- **Misc Service**: Health checks and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install xplainable pandas scikit-learn requests\n",
    "# Note: Install the refactored xplainable-client locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jtuppack/miniforge3/envs/xplainable-jupyter/lib/python3.10/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xplainable version: 1.3.0\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "from typing import Optional, Dict, Any, List\n",
    "from datetime import datetime\n",
    "\n",
    "# Xplainable core imports\n",
    "import xplainable as xp\n",
    "from xplainable.core.models import XClassifier\n",
    "from xplainable.core.optimisation.bayesian import XParamOptimiser\n",
    "from xplainable.preprocessing.pipeline import XPipeline\n",
    "from xplainable.preprocessing import transformers as xtf\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Refactored Xplainable Client imports (NEW!)\n",
    "from xplainable_client.client.client import XplainableClient\n",
    "from xplainable_client.client.base import XplainableAPIError\n",
    "from xplainable_client.client.py_models.models import CreateModelResponse\n",
    "from xplainable_client.client.py_models.deployments import CreateDeploymentResponse\n",
    "from xplainable_client.client.py_models.preprocessing import CreatePreprocessorResponse\n",
    "from xplainable_client.client.py_models.autotrain import DatasetSummary, TextGenConfig\n",
    "from xplainable_client.client.py_models.inference import PredictionResponse\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Xplainable version: {xp.__version__}\")\n",
    "print(f\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Refactored Xplainable Client\n",
    "\n",
    "The new client provides:\n",
    "- **Service separation**: `client.models`, `client.deployments`, etc.\n",
    "- **Type safety**: Full Pydantic model validation\n",
    "- **Error handling**: Detailed `XplainableAPIError` exceptions\n",
    "- **IDE support**: Complete autocompletion and type hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual API key\n",
    "API_KEY = \"\"  # Get from https://platform.xplainable.io/\n",
    "HOSTNAME = \"https://platform.xplainable.io\"\n",
    "\n",
    "try:\n",
    "    # Initialize the refactored client\n",
    "    client = XplainableClient(\n",
    "        api_key=API_KEY,\n",
    "        hostname=HOSTNAME\n",
    "    )\n",
    "    \n",
    "    # Display connection info\n",
    "    info = client.connection_info\n",
    "    print(f\"Connected successfully!\")\n",
    "    print(f\"User: {info['username']}\")\n",
    "    print(f\"Hostname: {info['hostname']}\")\n",
    "    print(f\"Xplainable Version: {info['xplainable_version']}\")\n",
    "    print(f\"Python Version: {info['python_version']}\")\n",
    "    \n",
    "    # Test service availability\n",
    "    services = ['models', 'deployments', 'preprocessing', 'collections', \n",
    "               'datasets', 'inference', 'gpt', 'autotrain', 'misc']\n",
    "    \n",
    "    print(f\"\\nAvailable services:\")\n",
    "    for service in services:\n",
    "        if hasattr(client, service):\n",
    "            print(f\"  ✓ {service}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {service}\")\n",
    "            \n",
    "except XplainableAPIError as e:\n",
    "    print(f\"API Error: {e.message}\")\n",
    "    if e.status_code == 401:\n",
    "        print(\"Please check your API key\")\n",
    "    elif e.status_code == 403:\n",
    "        print(\"Check your permissions\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {str(e)}\")\n",
    "    print(\"Make sure to replace 'your-api-key-here' with your actual API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Health Check\n",
    "\n",
    "Using the **Misc Service** to verify system connectivity and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Test server connectivity\n",
    "    print(\"Testing server connectivity...\")\n",
    "    \n",
    "    # Ping gateway\n",
    "    gateway_ping = client.misc.ping_gateway()\n",
    "    print(f\"Gateway ping: {'✓' if gateway_ping.success else '✗'} ({gateway_ping.response_time:.3f}s)\")\n",
    "    \n",
    "    # Try server ping (may not be available in all environments)\n",
    "    try:\n",
    "        server_ping = client.misc.ping_server()\n",
    "        print(f\"Server ping: {'✓' if server_ping.success else '✗'} ({server_ping.response_time:.3f}s)\")\n",
    "    except XplainableAPIError as e:\n",
    "        print(f\"Server ping: Not available ({e.status_code})\")\n",
    "    \n",
    "    # Get version information\n",
    "    version_info = client.misc.get_version_info()\n",
    "    print(f\"\\nVersion Information:\")\n",
    "    print(f\"  • Xplainable: {version_info.xplainable_version}\")\n",
    "    print(f\"  • Python: {version_info.python_version}\")\n",
    "    \n",
    "except XplainableAPIError as e:\n",
    "    print(f\"Health check warning: {e.message}\")\n",
    "except Exception as e:\n",
    "    print(f\"Health check failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Telco Churn Dataset\n",
    "\n",
    "Using the **Datasets Service** to explore available datasets and load the Telco data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to list available public datasets\n",
    "    print(\"Checking available public datasets...\")\n",
    "    datasets = client.datasets.list_datasets()\n",
    "    print(f\"Found {len(datasets)} public datasets:\")\n",
    "    \n",
    "    # Show first few datasets\n",
    "    for i, dataset in enumerate(datasets[:5]):\n",
    "        print(f\"  {i+1}. {dataset}\")\n",
    "    \n",
    "    if len(datasets) > 5:\n",
    "        print(f\"  ... and {len(datasets) - 5} more\")\n",
    "        \n",
    "except XplainableAPIError as e:\n",
    "    print(f\"Dataset service warning: {e.message}\")\n",
    "except Exception as e:\n",
    "    print(f\"Dataset service info: {str(e)}\")\n",
    "\n",
    "# Load the Telco dataset directly (primary method)\n",
    "print(\"\\nLoading IBM Telco Customer Churn dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv('https://xplainable-public-storage.syd1.digitaloceanspaces.com/example_data/telco_customer_churn.csv')\n",
    "    print(f\"Successfully loaded dataset with shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample data:\")\n",
    "    display(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset: {str(e)}\")\n",
    "    # Create sample data for demonstration\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    df = pd.DataFrame({\n",
    "        'CustomerID': [f'C{i:04d}' for i in range(100)],\n",
    "        'Gender': np.random.choice(['Male', 'Female'], 100),\n",
    "        'Tenure Months': np.random.randint(1, 72, 100),\n",
    "        'Monthly Charges': np.random.uniform(20, 120, 100),\n",
    "        'Churn Label': np.random.choice(['Yes', 'No'], 100)\n",
    "    })\n",
    "    print(f\"Created sample dataset with shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Powered Dataset Analysis with Autotrain\n",
    "\n",
    "Using the **Autotrain Service** to get AI-powered insights about our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for autotrain analysis\n",
    "print(\"Starting AI-powered dataset analysis...\")\n",
    "\n",
    "try:\n",
    "    # Save a sample of the dataset for analysis\n",
    "    sample_df = df.sample(min(1000, len(df)))  # Use sample for faster processing\n",
    "    temp_file = \"/tmp/telco_sample.csv\"\n",
    "    sample_df.to_csv(temp_file, index=False)\n",
    "    \n",
    "    # Configure AI text generation\n",
    "    textgen_config = TextGenConfig(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    print(f\"Analyzing dataset sample ({len(sample_df)} rows)...\")\n",
    "    \n",
    "    # Get AI-powered dataset summary\n",
    "    try:\n",
    "        summary = client.autotrain.summarize_dataset(\n",
    "            file_path=temp_file,\n",
    "            textgen_config=textgen_config\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset Analysis Complete!\")\n",
    "        print(f\"Columns: {len(summary.columns)}\")\n",
    "        print(f\"Rows: {summary.rows:,}\")\n",
    "        print(f\"Data Types: {len(set(summary.dtypes.values()))} unique types\")\n",
    "        \n",
    "        # Show missing values\n",
    "        missing_data = {k: v for k, v in summary.missing_values.items() if v > 0}\n",
    "        if missing_data:\n",
    "            print(f\"Missing Values: {missing_data}\")\n",
    "        else:\n",
    "            print(f\"No missing values detected\")\n",
    "            \n",
    "    except XplainableAPIError as e:\n",
    "        print(f\"Autotrain analysis not available: {e.message}\")\n",
    "        print(f\"Status: {e.status_code} - This is expected if autotrain endpoints are not fully implemented\")\n",
    "        \n",
    "        # Create manual summary for continuation\n",
    "        summary = DatasetSummary(\n",
    "            columns=list(df.columns),\n",
    "            rows=len(df),\n",
    "            dtypes=df.dtypes.astype(str).to_dict(),\n",
    "            missing_values=df.isnull().sum().to_dict()\n",
    "        )\n",
    "        print(f\"Manual summary created for continuation\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Autotrain service info: {str(e)}\")\n",
    "    # Continue with manual analysis\n",
    "    summary = DatasetSummary(\n",
    "        columns=list(df.columns),\n",
    "        rows=len(df),\n",
    "        dtypes=df.dtypes.astype(str).to_dict(),\n",
    "        missing_values=df.isnull().sum().to_dict()\n",
    "    )\n",
    "\n",
    "print(f\"\\nProceeding with data preprocessing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline\n",
    "\n",
    "Creating and managing preprocessing pipelines with the **Preprocessing Service**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "print(\"Setting up data preprocessing pipeline...\")\n",
    "\n",
    "# Convert target to binary\n",
    "df[\"Churn Label\"] = df[\"Churn Label\"].map({\"Yes\": 1, \"No\": 0})\n",
    "print(f\"Target variable converted to binary: {df['Churn Label'].value_counts().to_dict()}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "pipeline = XPipeline()\n",
    "\n",
    "# Add preprocessing stages\n",
    "pipeline.add_stages([\n",
    "    # Standardize text cases\n",
    "    {\"transformer\": xtf.ChangeCases(\n",
    "        columns=['City', 'Gender', 'Senior Citizen', 'Partner', 'Dependents',\n",
    "                'Phone Service', 'Multiple Lines', 'Internet Service',\n",
    "                'Online Security', 'Online Backup', 'Device Protection', 'Tech Support',\n",
    "                'Streaming TV', 'Streaming Movies', 'Contract', 'Paperless Billing',\n",
    "                'Payment Method'], \n",
    "        case=\"lower\"\n",
    "    )},\n",
    "    \n",
    "    # Condense low-frequency categories\n",
    "    {\"feature\": \"City\", \"transformer\": xtf.Condense(pct=0.25)},\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    {\"feature\": \"Monthly Charges\", \"transformer\": xtf.SetDType(to_type=\"float\")},\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    {\"transformer\": xtf.DropCols(\n",
    "        columns=[\n",
    "            'CustomerID',      # High cardinality\n",
    "            \"Total Charges\",   # Multicollinearity with Tenure\n",
    "            'Count',           # Single value\n",
    "            \"Country\",         # Single value  \n",
    "            \"State\",           # Single value\n",
    "            \"Zip Code\",        # High cardinality\n",
    "            \"Lat Long\",        # High cardinality\n",
    "            \"Latitude\",        # High cardinality\n",
    "            \"Longitude\",       # High cardinality\n",
    "            \"Churn Value\",     # Data leakage\n",
    "            \"Churn Score\",     # Data leakage\n",
    "            \"CLTV\",            # Data leakage\n",
    "            \"Churn Reason\",    # Data leakage\n",
    "        ]\n",
    "    )},\n",
    "])\n",
    "\n",
    "print(f\"Pipeline created with {len(pipeline.stages)} stages\")\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Applying preprocessing pipeline...\")\n",
    "df_transformed = pipeline.fit_transform(df)\n",
    "print(f\"Preprocessing complete!\")\n",
    "print(f\"Original shape: {df.shape} → Processed shape: {df_transformed.shape}\")\n",
    "\n",
    "# Display processed data sample\n",
    "print(\"\\nProcessed data sample:\")\n",
    "display(df_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Preprocessing Pipeline to Cloud\n",
    "\n",
    "Using the **Preprocessing Service** with proper error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Saving preprocessing pipeline to Xplainable Cloud...\")\n",
    "    \n",
    "    # Create preprocessor with type-safe response\n",
    "    preprocessor_id, version_id = client.preprocessing.create_preprocessor(\n",
    "        preprocessor_name=f\"Telco Churn Preprocessing - v{xp.__version__} - New Client 2\",\n",
    "        preprocessor_description=\"Complete preprocessing pipeline for IBM Telco Churn Dataset with refactored client\",\n",
    "        pipeline=pipeline,\n",
    "        df=df\n",
    "    )\n",
    "    \n",
    "    print(f\"Preprocessor saved successfully!\")\n",
    "    print(f\"Preprocessor ID: {preprocessor_id}\")\n",
    "    print(f\"Version ID: {version_id}\")\n",
    "    \n",
    "    # Test loading the preprocessor back\n",
    "    print(\"\\nTesting preprocessor reload...\")\n",
    "    try:\n",
    "        loaded_pipeline = client.preprocessing.load_preprocessor(\n",
    "            preprocessor_id=preprocessor_id,\n",
    "            version_id=version_id\n",
    "        )\n",
    "        print(f\"Preprocessor reloaded successfully!\")\n",
    "        print(f\"Stages: {len(loaded_pipeline.stages)}\")\n",
    "        \n",
    "    except XplainableAPIError as e:\n",
    "        print(f\"Preprocessor reload warning: {e.message} (Status: {e.status_code})\")\n",
    "        \n",
    "except XplainableAPIError as e:\n",
    "    print(f\"Preprocessor save warning: {e.message}\")\n",
    "    print(f\"Status: {e.status_code} - Continuing with local pipeline\")\n",
    "    preprocessor_id, version_id = \"local-pipeline\", \"local-version\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Preprocessing service info: {str(e)}\")\n",
    "    preprocessor_id, version_id = \"local-pipeline\", \"local-version\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Optimization\n",
    "\n",
    "Training an XClassifier with hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "print(\"Preparing training data...\")\n",
    "\n",
    "X = df_transformed.drop(columns=['Churn Label'])\n",
    "y = df_transformed['Churn Label']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Data split complete:\")\n",
    "print(f\"  Training: {X_train.shape[0]:,} samples\")\n",
    "print(f\"  Testing: {X_test.shape[0]:,} samples\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Class balance: {dict(y_train.value_counts())}\")\n",
    "\n",
    "# Hyperparameter optimization\n",
    "print(\"\\nStarting hyperparameter optimization...\")\n",
    "try:\n",
    "    opt = XParamOptimiser()\n",
    "    params = opt.optimise(X_train, y_train)\n",
    "    print(f\"Optimization complete!\")\n",
    "    print(f\"Best parameters: {list(params.keys())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Optimization failed, using defaults: {str(e)}\")\n",
    "    params = {}\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining XClassifier...\")\n",
    "model = XClassifier(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Model training complete!\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explainability\n",
    "\n",
    "Generate model explanations and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model explanations\n",
    "print(\"Generating model explanations...\")\n",
    "\n",
    "try:\n",
    "    # Generate explanation visualization\n",
    "    explanation = model.explain()\n",
    "    print(f\"Model explanation generated successfully!\")\n",
    "    \n",
    "    # Display explanation (this will show the interactive chart)\n",
    "    display(explanation)\n",
    "    \n",
    "    # Get feature importances for analysis\n",
    "    if hasattr(model, 'feature_importances'):\n",
    "        importances = model.feature_importances\n",
    "        print(f\"\\nTop 5 Most Important Features:\")\n",
    "        for i, (feature, importance) in enumerate(sorted(importances.items(), key=lambda x: x[1], reverse=True)[:5]):\n",
    "            print(f\"  {i+1}. {feature}: {importance:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Explanation generation failed: {str(e)}\")\n",
    "    print(f\"Model still trained successfully and ready for deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model to Cloud\n",
    "\n",
    "Using the **Models Service** with type-safe operations and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Saving model to Xplainable Cloud...\")\n",
    "    \n",
    "    # Create model with type-safe response\n",
    "    model_id, model_version_id = client.models.create_model(\n",
    "        model=model,\n",
    "        model_name=f\"Telco Churn Classifier - v{xp.__version__}\",\n",
    "        model_description=\"Advanced churn prediction model using XClassifier with the refactored client\",\n",
    "        x=X_train,\n",
    "        y=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"Model saved successfully!\")\n",
    "    print(f\"Model ID: {model_id}\")\n",
    "    print(f\"Version ID: {model_version_id}\")\n",
    "    \n",
    "    # Test model information retrieval\n",
    "    try:\n",
    "        print(\"\\nRetrieving model information...\")\n",
    "        model_info = client.models.get_model(model_id)\n",
    "        print(f\"Model info retrieved:\")\n",
    "        print(f\"  Name: {model_info.model_name}\")\n",
    "        print(f\"  Description: {model_info.model_description}\")\n",
    "        \n",
    "    except XplainableAPIError as e:\n",
    "        print(f\"Model info retrieval warning: {e.message} (Status: {e.status_code})\")\n",
    "        \n",
    "except XplainableAPIError as e:\n",
    "    print(f\"Model save warning: {e.message}\")\n",
    "    print(f\"Status: {e.status_code} - Continuing with local model\")\n",
    "    model_id, model_version_id = \"local-model\", \"local-version\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Models service info: {str(e)}\")\n",
    "    model_id, model_version_id = \"local-model\", \"local-version\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Using the **Deployments Service** with comprehensive error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Deploying model...\")\n",
    "    \n",
    "    # Deploy the model with type-safe response\n",
    "    deployment_response: CreateDeploymentResponse = client.deployments.deploy(model_version_id)\n",
    "    deployment_id = deployment_response.deployment_id\n",
    "    \n",
    "    print(f\"Model deployed successfully!\")\n",
    "    print(f\"Deployment ID: {deployment_id}\")\n",
    "    \n",
    "    # Generate deployment key\n",
    "    try:\n",
    "        print(\"\\nGenerating deployment key...\")\n",
    "        deploy_key = client.deployments.generate_deploy_key(\n",
    "            deployment_id=deployment_id,\n",
    "            description=\"Telco Churn Prediction API - Refactored Client Demo\",\n",
    "            days_until_expiry=30\n",
    "        )\n",
    "        \n",
    "        print(f\"Deployment key generated!\")\n",
    "        print(f\"Key: {str(deploy_key)[:16]}...[truncated]\")\n",
    "        print(f\"Expires in: 30 days\")\n",
    "        \n",
    "    except XplainableAPIError as e:\n",
    "        print(f\"Deploy key generation warning: {e.message} (Status: {e.status_code})\")\n",
    "        deploy_key = None\n",
    "    \n",
    "    # List deployments\n",
    "    try:\n",
    "        print(\"\\nListing team deployments...\")\n",
    "        deployments = client.deployments.list_deployments()\n",
    "        print(f\"Found {len(deployments)} deployment(s):\")\n",
    "        \n",
    "        for i, dep in enumerate(deployments[:3]):  # Show first 3\n",
    "            print(f\"  {i+1}. {dep.deployment_id} (Active: {dep.active})\")\n",
    "            \n",
    "    except XplainableAPIError as e:\n",
    "        print(f\"Deployment listing warning: {e.message} (Status: {e.status_code})\")\n",
    "        \n",
    "except XplainableAPIError as e:\n",
    "    print(f\"Deployment warning: {e.message}\")\n",
    "    print(f\"Status: {e.status_code}\")\n",
    "    \n",
    "    if e.status_code == 403:\n",
    "        print(f\"This might be due to deployment quota limits\")\n",
    "    elif e.status_code == 404:\n",
    "        print(f\"Model version might not exist on the server\")\n",
    "    \n",
    "    deployment_id = None\n",
    "    deploy_key = None\n",
    "except Exception as e:\n",
    "    print(f\"Deployment service info: {str(e)}\")\n",
    "    deployment_id = None\n",
    "    deploy_key = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference and Predictions\n",
    "\n",
    "Using the **Inference Service** for predictions and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Testing model inference...\")\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_sample = X_test.sample(3)  # Get 3 random samples\n",
    "    print(f\"Testing with {len(test_sample)} samples\")\n",
    "    \n",
    "    # Method 1: Local predictions (always available)\n",
    "    print(\"\\nLocal Inference:\")\n",
    "    local_predictions = model.predict(test_sample)\n",
    "    local_probabilities = model.predict_proba(test_sample)\n",
    "    \n",
    "    print(f\"Local predictions: {local_predictions.tolist()}\")\n",
    "    print(f\"Churn probabilities: {[f'{p:.3f}' for p in local_probabilities[:, 1]]}\")\n",
    "    \n",
    "    # Method 2: File-based API inference (using the refactored client)\n",
    "    if model_id and model_version_id:\n",
    "        try:\n",
    "            print(\"\\nFile-based API Inference:\")\n",
    "            \n",
    "            # Save test sample to CSV file for API prediction\n",
    "            temp_file = \"/tmp/test_sample.csv\"\n",
    "            test_sample.to_csv(temp_file, index=False)\n",
    "            \n",
    "            # Make predictions via API using the predict method\n",
    "            api_predictions = client.inference.predict(\n",
    "                filename=temp_file,\n",
    "                model_id=model_id,\n",
    "                version_id=model_version_id,\n",
    "                threshold=0.5,\n",
    "                delimiter=\",\"\n",
    "            )\n",
    "            \n",
    "            print(f\"API predictions successful!\")\n",
    "            print(f\"API response type: {type(api_predictions)}\")\n",
    "            \n",
    "            # Clean up temp file\n",
    "            import os\n",
    "            try:\n",
    "                os.remove(temp_file)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"API inference warning: {str(e)}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\nAPI inference skipped (no model ID available)\")\n",
    "    \n",
    "    # Display prediction results\n",
    "    print(\"\\nPrediction Summary:\")\n",
    "    for i, (idx, row) in enumerate(test_sample.iterrows()):\n",
    "        local_pred = local_predictions[i]\n",
    "        local_prob = local_probabilities[i, 1]\n",
    "        \n",
    "        print(f\"  Sample {i+1}:\")\n",
    "        print(f\"    Prediction: {'Churn' if local_pred == 1 else 'No Churn'}\")\n",
    "        print(f\"    Probability: {local_prob:.3f}\")\n",
    "        print(f\"    Monthly Charges: ${row['Monthly Charges']:.2f}\")\n",
    "        print(f\"    Tenure: {row['Tenure Months']} months\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Inference service info: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Powered Insights with GPT Service\n",
    "\n",
    "Using the **GPT Service** to generate intelligent reports and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Generating AI-powered insights...\")\n",
    "    \n",
    "    # Generate model report\n",
    "    try:\n",
    "        print(\"Creating model report...\")\n",
    "        report = client.gpt.generate_report(\n",
    "            model_id=model_id,\n",
    "            version_id=model_version_id,\n",
    "            target_description=\"Customer churn likelihood (1 = will churn, 0 = will stay)\",\n",
    "            project_objective=\"Identify customers at risk of leaving to improve retention strategies\",\n",
    "            max_features=10,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        print(f\"AI report generated!\")\n",
    "        print(f\"Report length: {len(report.report):,} characters\")\n",
    "        \n",
    "        if report.key_insights:\n",
    "            print(f\"Key insights: {len(report.key_insights)}\")\n",
    "        \n",
    "        # Display report (truncated)\n",
    "        print(f\"\\nReport Preview:\")\n",
    "        print(f\"{report.report[:500]}...\")\n",
    "        \n",
    "    except XplainableAPIError as e:\n",
    "        print(f\"GPT report warning: {e.message} (Status: {e.status_code})\")\n",
    "    \n",
    "    # Generate model explanation\n",
    "    try:\n",
    "        print(\"\\nCreating natural language explanation...\")\n",
    "        explanation = client.gpt.explain_model(\n",
    "            model_id=model_id,\n",
    "            version_id=model_version_id,\n",
    "            language=\"en\",\n",
    "            detail_level=\"medium\"\n",
    "        )\n",
    "        \n",
    "        print(f\"AI explanation generated!\")\n",
    "        print(f\"\\nModel Explanation:\")\n",
    "        print(f\"{explanation.explanation[:400]}...\")\n",
    "        \n",
    "    except XplainableAPIError as e:\n",
    "        print(f\"GPT explanation warning: {e.message} (Status: {e.status_code})\")\n",
    "    \n",
    "    # Manual insights (always available)\n",
    "    print(f\"\\nManual Model Analysis:\")\n",
    "    print(f\"  Training Data: {len(X_train):,} samples\")\n",
    "    print(f\"  Features Used: {X_train.shape[1]}\")\n",
    "    print(f\"  Class Distribution: {dict(y_train.value_counts())}\")\n",
    "    \n",
    "    if hasattr(model, 'feature_importances'):\n",
    "        top_feature = max(model.feature_importances.items(), key=lambda x: x[1])\n",
    "        print(f\"  Most Important Feature: {top_feature[0]} ({top_feature[1]:.3f})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"GPT service info: {str(e)}\")\n",
    "    print(f\"Manual analysis available above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections and Organization\n",
    "\n",
    "Using the **Collections Service** to organize models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Creating model collection...\")\n",
    "    \n",
    "    # Create a collection for churn models\n",
    "    try:\n",
    "        collection_response = client.collections.create_collection(\n",
    "            model_id=model_id,\n",
    "            name=\"Churn Prediction Models\",\n",
    "            description=\"Collection of customer churn prediction models for retention strategies\"\n",
    "        )\n",
    "        \n",
    "        collection_id = collection_response.collection_id\n",
    "        print(f\"Collection created!\")\n",
    "        print(f\"Collection ID: {collection_id}\")\n",
    "        \n",
    "        # Create scenarios using sample data\n",
    "        try:\n",
    "            print(\"\\nCreating prediction scenarios...\")\n",
    "            \n",
    "            # First, get the model partitions to get a valid partition_id\n",
    "            try:\n",
    "                partitions = client.models.list_model_version_partitions(model_version_id)\n",
    "                if partitions and len(partitions.get('partitions', [])) > 0:\n",
    "                    partition_id = partitions['partitions'][0]['partition_id']\n",
    "                    print(f\"  Using partition ID: {partition_id}\")\n",
    "                else:\n",
    "                    print(\"  No partitions found, skipping scenario creation\")\n",
    "                    raise Exception(\"No valid partition found\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Could not get partitions: {str(e)}\")\n",
    "                raise\n",
    "            \n",
    "            # Take 5 random samples from test data for scenarios\n",
    "            sample_scenarios = X_test.sample(5)\n",
    "            \n",
    "            # Create scenario data with predictions (one by one to match API structure)\n",
    "            created_scenarios = []\n",
    "            for idx, (_, row) in enumerate(sample_scenarios.iterrows()):\n",
    "                try:\n",
    "                    # Get prediction for this sample - convert to DataFrame for consistency\n",
    "                    sample_df = row.to_frame().T  # Convert Series to single-row DataFrame\n",
    "                    sample_prediction = model.predict(sample_df)\n",
    "                    sample_probability = model.predict_proba(sample_df)\n",
    "                    \n",
    "                    # Handle both scalar and array predictions\n",
    "                    if hasattr(sample_prediction, '__len__') and len(sample_prediction) > 0:\n",
    "                        pred_value = float(sample_prediction[0])\n",
    "                    else:\n",
    "                        pred_value = float(sample_prediction)\n",
    "                    \n",
    "                    # Handle probability array\n",
    "                    if hasattr(sample_probability, '__len__') and len(sample_probability) > 0:\n",
    "                        if hasattr(sample_probability[0], '__len__') and len(sample_probability[0]) > 1:\n",
    "                            prob_value = float(sample_probability[0][1])  # Churn probability\n",
    "                        else:\n",
    "                            prob_value = float(sample_probability[0])\n",
    "                    else:\n",
    "                        prob_value = float(sample_probability)\n",
    "                    \n",
    "                    # Determine risk level\n",
    "                    risk_level = \"High\" if prob_value > 0.7 else \"Medium\" if prob_value > 0.3 else \"Low\"\n",
    "                    \n",
    "                    # Create individual scenario matching API structure\n",
    "                    scenario_data = {\n",
    "                        \"version_id\": model_version_id,\n",
    "                        \"partition_id\": partition_id,  # Use the actual partition ID\n",
    "                        \"scenario\": row.to_dict(),  # The actual feature data\n",
    "                        \"score\": pred_value,\n",
    "                        \"proba\": prob_value,\n",
    "                        \"multiplier\": 1.0,  # Default multiplier value\n",
    "                        \"support\": 0,  # Default support value\n",
    "                        \"notes\": f\"Customer Scenario {idx + 1}: {row['Contract']} contract, ${row['Monthly Charges']:.2f}/month, Risk: {risk_level}\"\n",
    "                    }\n",
    "                    \n",
    "                except Exception as pred_error:\n",
    "                    print(f\"  Prediction error for scenario {idx + 1}: {str(pred_error)}\")\n",
    "                    # Create a basic scenario without predictions\n",
    "                    scenario_data = {\n",
    "                        \"version_id\": model_version_id,\n",
    "                        \"partition_id\": partition_id,  # Use the actual partition ID\n",
    "                        \"scenario\": row.to_dict(),\n",
    "                        \"score\": 0.0,\n",
    "                        \"proba\": 0.5,  # Default neutral probability\n",
    "                        \"multiplier\": 1.0,  # Default multiplier value\n",
    "                        \"support\": 0,  # Default support value\n",
    "                        \"notes\": f\"Customer Scenario {idx + 1}: {row['Contract']} contract, ${row['Monthly Charges']:.2f}/month (prediction failed)\"\n",
    "                    }\n",
    "                \n",
    "                # Create single scenario (since API expects individual scenarios)\n",
    "                try:\n",
    "                    created_scenario = client.collections.create_scenarios(\n",
    "                        collection_id=collection_id,\n",
    "                        scenarios=[scenario_data]  # Single scenario in list\n",
    "                    )\n",
    "                    created_scenarios.extend(created_scenario)\n",
    "                    print(f\"  Created scenario {idx + 1}: Risk Level {risk_level}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Failed to create scenario {idx + 1}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"Created {len(created_scenarios)} scenarios total!\")\n",
    "                \n",
    "        except XplainableAPIError as e:\n",
    "            print(f\"Scenario creation warning: {e.message} (Status: {e.status_code})\")\n",
    "        \n",
    "        # List team collections\n",
    "        try:\n",
    "            print(f\"\\nListing team collections...\")\n",
    "            collections = client.collections.get_team_collections()\n",
    "            print(f\"Found {len(collections)} team collection(s)\")\n",
    "            \n",
    "            for collection in collections[-3:]:  # Show last 3\n",
    "                print(f\"  {collection.get('name', 'Unnamed')} (ID: {collection.get('id', 'unknown')})\")\n",
    "                \n",
    "        except XplainableAPIError as e:\n",
    "            print(f\"Collection listing warning: {e.message} (Status: {e.status_code})\")\n",
    "        \n",
    "        # Get collection scenarios\n",
    "        try:\n",
    "            print(f\"\\nRetrieving collection scenarios...\")\n",
    "            scenarios = client.collections.get_collection_scenarios(collection_id)\n",
    "            print(f\"Found {len(scenarios)} scenario(s) in collection\")\n",
    "            \n",
    "        except XplainableAPIError as e:\n",
    "            print(f\"Scenario retrieval warning: {e.message} (Status: {e.status_code})\")\n",
    "            \n",
    "    except XplainableAPIError as e:\n",
    "        print(f\"Collection creation warning: {e.message} (Status: {e.status_code})\")\n",
    "        collection_id = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Collections service info: {str(e)}\")\n",
    "    collection_id = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xplainable-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
